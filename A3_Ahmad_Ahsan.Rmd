---
title: "Decision Tree and Naïve Cross-validation"
author: "Ahsan Ahmad"
date: "March 16, 2024"
output: 
  html_document:
    number_sections: yes
    toc: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE)
```

# Libraries

```{r library}
library(tidyverse)
library(e1071)
library(psych)
library(caret)
library(rminer)
library(rmarkdown)
library(matrixStats)
library(knitr)
library(scatterplot3d)
library(C50)
library(tictoc)
tic()
```

# Set up, Data import, and Preparation

Before proceeding with the analysis, it's essential to set up the working directory and import the data from a CSV file named 'CD_additional_modified.csv'. This code chunk also inspects the structure of the dataset and converts string data into factors for further analysis.

```{r Set up, data import and inspection}

# Setting up working directory and importing data from a csv file

cloud_wd <- getwd()
setwd(cloud_wd)

CD_balance <- read.csv(file = "CD_additional_modified.csv", stringsAsFactors = TRUE)
str(CD_balance)

# Looking at the structure and summary of the data

str(CD_balance)
summary(CD_balance)

set.seed(100)  #Creating data partition by using 70% of data for the training set and 30% for the test set
CD_Train <- createDataPartition(CD_balance$y, p=0.7, list=FALSE)

length(CD_Train)
class(CD_Train)

train_set <- CD_balance[CD_Train,]  #spliting the dataset using the indexes
test_set <- CD_balance[-CD_Train,]

train_set %>% nrow()  #Showing number of rows in both train and test sets
test_set %>% nrow()

train_set %>% select(y) %>% table() #Showing the proportion of yes and no in y for training set
train_set %>% select(y) %>% table() %>% prop.table() %>% round(2) * 100

test_set %>% select(y) %>% table() #Showing the proportion of yes and no in y for training set
test_set %>% select(y) %>% table() %>% prop.table() %>% round(2) * 100


# Showing the count value and percentage value of y and it's levels for whole dataset

CD_balance %>% select(y) %>% table()
CD_balance %>% select(y) %>% table() %>% prop.table() %>% round(2) * 100

```


# Simple Decision Tree Training and Testing

In this code chunk, we train and evaluate decision tree models using the C5.0 algorithm. We start by training a default decision tree model using the training set and then evaluate its performance on both the training and test sets. We generate predictions, create confusion matrices, and calculate accuracy metrics for both sets.

Next, we create a second decision tree model with a lower CF (confidence factor) value of 0.05 to potentially reduce overfitting. We plot the non-complex tree generated by this model and evaluate its performance on the training and test sets as well.


```{r Decision Trees}

# Using the train set to train C5.0 model with default setting
decision_tree_model1 <- C5.0(y ~ ., data = train_set)
decision_tree_model1
summary(decision_tree_model1)

# Generating prediction for the default model for both the train and test set

model_1_train_predictions <- predict(decision_tree_model1,train_set)
model_1_test_predictions <- predict(decision_tree_model1,test_set)

# Creating confusion matrix for both the train and test set data

Confusion_Matrix_train_1 <- table(predicted = model_1_train_predictions,
                                  observed = train_set$y)
Confusion_Matrix_train_1

Confusion_Matrix_test_1 <- table(predicted = model_1_test_predictions,
                                  observed = test_set$y)
Confusion_Matrix_test_1

# Checking for accuracy of how the model performed on both the train and test datasets

mmetric(train_set$y, model_1_train_predictions, metric="ACC")
mmetric(test_set$y, model_1_test_predictions, metric = "ACC")

# Creating a second decision tree model with a lower CF value of 0.05

decision_tree_model2 <- C5.0(formula = y ~ .,data = train_set, control = C5.0Control(CF = 0.05, earlyStopping = FALSE, noGlobalPruning = FALSE))
decision_tree_model2
summary(decision_tree_model2)

# Plotting the non-complex tree with a CF value of 0.05

plot(decision_tree_model2)

# Generating prediction for the new model with CF=0.05 for both the train and test set

model_2_train_predictions <- predict(decision_tree_model2,train_set)
model_2_test_predictions <- predict(decision_tree_model2,test_set)

# Creating confusion matrix for both the train and test set data for the new model

Confusion_Matrix_train_2 <- table(predicted = model_2_train_predictions,
                                  observed = train_set$y)
Confusion_Matrix_train_2

Confusion_Matrix_test_2 <- table(predicted = model_2_test_predictions,
                                  observed = test_set$y)
Confusion_Matrix_test_2

# Checking for accuracy of how the model performed on both the train and test datasets

mmetric(train_set$y, model_2_train_predictions, metric="ACC")
mmetric(test_set$y, model_2_test_predictions, metric = "ACC")

```


# Simple Naïve Bayes Model Training and Testing

In this code chunk we will create and assesses Naive Bayes models using the training dataset. The models are then used to generate predictions for both training and test sets, followed by the creation of confusion matrices to evaluate performance. Additionally, the C5imp() function identifies the least important predictors from a previous decision tree model. Finally, a modified Naive Bayes model is constructed, excluding the identified predictor, and its performance is evaluated.

```{r NB model building}

# Creating a simple Naive Bayes Model from the training set

naive_model1 <- naiveBayes(y ~ ., data = train_set)
naive_model1
summary(naive_model1)

# Generating prediction for the simple naive bayes model for both the train and test set

naive_model_1_train_predictions <- predict(naive_model1,train_set)
naive_model_1_test_predictions <- predict(naive_model1,test_set)

# Creating confusion matrix for both the train and test set data

Naive_Confusion_Matrix_train_1 <- table(predicted = naive_model_1_train_predictions,
                                  observed = train_set$y)
Naive_Confusion_Matrix_train_1

Naive_Confusion_Matrix_test_1 <- table(predicted = naive_model_1_test_predictions,
                                  observed = test_set$y)
Naive_Confusion_Matrix_test_1

# Checking for accuracy of how the model performed on both the train and test datasets

mmetric(train_set$y, naive_model_1_train_predictions, metric="ACC")
mmetric(test_set$y, naive_model_1_test_predictions, metric = "ACC")

# Using the C5imp() function to see the least important predictors from the previous decision tree model 1

C5imp(decision_tree_model1)

# Creating a simple Naive Bayes Model from the training set without the predictor previous

naive_model2 <- naiveBayes(y ~ . - previous, data = train_set)
naive_model2
summary(naive_model2)

# Generating prediction for the new naive bayes model for both the train and test set

naive_model_2_train_predictions <- predict(naive_model2,train_set)
naive_model_2_test_predictions <- predict(naive_model2,test_set)

# Creating confusion matrix for both the train and test set data

Naive_Confusion_Matrix_train_2 <- table(predicted = naive_model_2_train_predictions,
                                  observed = train_set$y)
Naive_Confusion_Matrix_train_2

Naive_Confusion_Matrix_test_2 <- table(predicted = naive_model_2_test_predictions,
                                  observed = test_set$y)
Naive_Confusion_Matrix_test_2

# Checking for accuracy of how the model performed on both the train and test datasets

mmetric(train_set$y, naive_model_2_train_predictions, metric="ACC")
mmetric(test_set$y, naive_model_2_test_predictions, metric = "ACC")

```


The count of true 'yes' have increased from 194 to 205 in the train set and from 75 to 81 in the test set after removing the predictor 'previous'. Accuracy of the sets have also increased slightly.


# Create a Named Cross-validation Function – cv_function

The cv_function performs k-fold cross-validation on a dataset using a specified classification algorithm. It divides the data into training and testing sets for each fold, trains the model, and evaluates its performance using specified metrics. The results are aggregated to compute mean and standard deviation values, providing a concise summary of model performance across the folds.

```{r Define cv_function}

cv_function <- function(df, target, nFolds, seedVal, classification, metrics_list)
{
  set.seed(seedVal)
  folds = createFolds(df[,target],nFolds) 
  
  cv_results <- lapply(folds, function(x)
  { 
    train <- df[-x,-target]
    test  <- df[x,-target]
    
    train_target <- df[-x,target]
    test_target <- df[x,target]
    
    classification_model <- classification(train,train_target) 
    
    pred<- predict(classification_model,test)
    
    return(mmetric(test_target,pred,c("ACC","PRECISION","TPR","F1")))
    
  })
  
  cv_results_m <- as.matrix(as.data.frame(cv_results))
  
  cv_mean<- as.matrix(rowMeans(cv_results_m))
  
  colnames(cv_mean) <- "Mean"
  
  cv_sd <- as.matrix(rowSds(cv_results_m))
  
  colnames(cv_sd) <- "Sd"
  
  cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
  
  kable(cv_all,digits=2)
}
```


# 5-fold and 10-fold C5.0 and naive Bayes evaluation performance with cv_function

The code segment runs cross-validation (CV) evaluations for C5.0 and Naive Bayes classification models on a dataset. It first defines parameters for 5-fold and 10-fold CV, then utilizes the cv_function to evaluate model performance using accuracy, precision, true positive rate (TPR), and F1 score metrics. The process is repeated for both C5.0 and Naive Bayes models, providing insights into their performance across different folds.

```{r Run Cross Validations}

# Using the CV_Function to evaluate 5-fold and 10-fold C5.0 model evaluation performance.

# 5-fold, C5.0

df <- CD_balance
target <- 21
nFolds <- 5
seedVal <- 500
assign("classification", C5.0)
metrics_list <- c("ACC","PRECISION","TPR","F1")

cv_function(df, target, nFolds, seedVal, classification, metrics_list)

# 10-fold, C5.0

df <- CD_balance
target <- 21
nFolds <- 10
seedVal <- 500
assign("classification", C5.0)
metrics_list <- c("ACC","PRECISION","TPR","F1")

cv_function(df, target, nFolds, seedVal, classification, metrics_list)

# Using the CV_Function to evaluate 5-fold and 10-fold naive Bayes model evaluation performance.

# 5-fold, naive Bayes

df <- CD_balance
target <- 21
nFolds <- 5
seedVal <- 500
assign("classification", naiveBayes)
metrics_list <- c("ACC","PRECISION","TPR","F1")

cv_function(df, target, nFolds, seedVal, classification, metrics_list)

# 10-fold, naive Bayes

df <- CD_balance
target <- 21
nFolds <- 10
seedVal <- 500
assign("classification", naiveBayes)
metrics_list <- c("ACC","PRECISION","TPR","F1")

cv_function(df, target, nFolds, seedVal, classification, metrics_list)


```


# Reflections and Findings

The accuracy for the default decision tree model has a significant decrease from 93.34% to 90.60% suggesting that the model was probably a little overfitting on the train data set but not a lot.

For the 2nd decision tree model with a low CF value of 0.05, the Accuracy for both the train and testing data set is almost the same with only a decrease of 0.9% and the testing set accuracy is comparable with the default model as well with a value of 90.04%.

Moving on to the Naive Bayes models. For the first simple model the Testing and Training set predictions have almost the same accuracy with both having a difference of only about 0.5% with Training set having an accuracy of 87.73% and Testing set having an accuracy of 87.29% which is considerably low when compared to the test set accuracy of decision tree models having an accuracy of 90%. 

This is due to the less complex nature of the naive bayes as it takes into account all the predictors rather than decision trees using C5.0 which has considers some predictors to be more important than others and looks for patterns and splits while naive bayes is mostly based on conditional probabilities of the predictors.

The count of true 'yes' have increased from 194 to 205 in the train set and from 75 to 81 in the test set after removing the predictor 'previous'. Accuracy of the sets have also increased slightly.

Upon doing cross validation for these models, the average accuracy remains same with 90% for decision tree models and around 87% for the naive bayes models suggesting that the model performance is stable and is not highly specific to the specific folds used in cross-validation. Hence, we can conclude that the models have generalized well to the data and there is very little over-fitting.
